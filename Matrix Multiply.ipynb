{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=torch.rand([10,56,85])\n",
    "m2=torch.rand([10,85,56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7 µs ± 1.51 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 56, 56])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "torch.matmul(m1,m2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matmul(m1,m2):\n",
    "    out=torch.zeros([len(m1),len(m1[0]),len(m2[0][0])])\n",
    "    for i in range(0,len(m1)):\n",
    "        for ix in range(len(m1[0])):\n",
    "            for iy in range(len(m2[0][0])):\n",
    "                out[i,ix,iy]=(m1[i][ix]*m2[i][:,iy]).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.6 µs ± 816 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.06 s ± 8.95 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "%timeit naive_matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_naive_matmul(m1,m2):\n",
    "    out=torch.zeros([len(m1),len(m1[0]),len(m2[0][0])])\n",
    "    for i in range(0,len(m1)):\n",
    "        for ix in range(len(m1[0])):\n",
    "                out[i,ix]=(m1[i][ix][:,None]*m2[i]).sum(dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.2 µs ± 3.86 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.06 s ± 3.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "26.7 ms ± 30.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "%timeit naive_matmul(m1,m2)\n",
    "%timeit less_naive_matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesser_naive_matmul(m1,m2):\n",
    "    out=torch.zeros([len(m1),len(m1[0]),len(m2[0][0])])\n",
    "    for i in range(0,len(m1)):\n",
    "        (m1[i].unsqueeze(-1)*m2[i]).sum(-2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.6 µs ± 3.05 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.07 s ± 2.77 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "26.7 ms ± 18.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "3.39 ms ± 946 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "%timeit naive_matmul(m1,m2)\n",
    "%timeit less_naive_matmul(m1,m2)\n",
    "%timeit lesser_naive_matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought is would be a good idea to move all the way down to a fully broadcasted version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_matmul(m1,m2):\n",
    "    return (m1[...,None]*m2[:][:,None]).sum(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.8 µs ± 3.11 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.06 s ± 1.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "26.6 ms ± 25.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "3.38 ms ± 993 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.02 ms ± 1.39 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "%timeit naive_matmul(m1,m2)\n",
    "%timeit less_naive_matmul(m1,m2)\n",
    "%timeit lesser_naive_matmul(m1,m2)\n",
    "%timeit my_matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought changing m1 to shape [10, 54, 1, 85] and [10, 1, 56, 85] to allow for [85]*[56,85] would inprove perf. This was not the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_matmul(m1,m2):\n",
    "    return (m1[:,:,None]*m2.transpose(1,2)[:,None]).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.2 µs ± 1.58 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.07 s ± 1.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "26.3 ms ± 52.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "3.38 ms ± 913 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.02 ms ± 4.86 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.03 ms ± 3.73 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "%timeit naive_matmul(m1,m2)\n",
    "%timeit less_naive_matmul(m1,m2)\n",
    "%timeit lesser_naive_matmul(m1,m2)\n",
    "%timeit my_matmul(m1,m2)\n",
    "%timeit alt_matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.6 µs ± 275 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.02 s ± 1.03 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "25.4 ms ± 54 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "3.28 ms ± 1.53 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2.9 ms ± 1.14 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2.9 ms ± 1.41 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "118 µs ± 417 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "%timeit naive_matmul(m1,m2)\n",
    "%timeit less_naive_matmul(m1,m2)\n",
    "%timeit lesser_naive_matmul(m1,m2)\n",
    "%timeit my_matmul(m1,m2)\n",
    "%timeit alt_matmul(m1,m2)\n",
    "%timeit torch.einsum('bik,bkj->bij',m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.1 µs ± 728 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.02 s ± 2.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "25.4 ms ± 41.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "3.28 ms ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2.9 ms ± 1.61 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2.91 ms ± 1.44 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "118 µs ± 191 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "89.8 µs ± 990 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(m1,m2)\n",
    "%timeit naive_matmul(m1,m2)\n",
    "%timeit less_naive_matmul(m1,m2)\n",
    "%timeit lesser_naive_matmul(m1,m2)\n",
    "%timeit my_matmul(m1,m2)\n",
    "%timeit alt_matmul(m1,m2)\n",
    "%timeit torch.einsum('bik,bkj->bij',m1,m2)\n",
    "%timeit m1@m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Einsums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to get a basic understanding of einsums, as they seem very powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1=torch.rand([10,7,5])\n",
    "e2=torch.rand([10,5,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matmul\n",
    "torch.einsum('bik,bkj->bij',e1,e2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('bik->bki',e1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added deminsion instead of adding up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7, 9, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('bik,bkj->bijk',e1,e2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pointless but interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7, 5, 10, 5, 9])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('abc,def->abcdef',e1,e2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load torch.sum??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _VariableFunctions.sum>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/molly/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/__init__.py'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -n torch.is_tensor\n",
    "def is_tensor(obj):\n",
    "    r\"\"\"Returns True if `obj` is a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "        obj (Object): Object to test\n",
    "    \"\"\"\n",
    "    return isinstance(obj, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -n torch.unique\n",
    "def unique(input, sorted=False, return_inverse=False, dim=None):\n",
    "    r\"\"\"Returns the unique scalar elements of the input tensor as a 1-D tensor.\n",
    "\n",
    "    Arguments:\n",
    "        input (Tensor): the input tensor\n",
    "        sorted (bool): Whether to sort the unique elements in ascending order\n",
    "            before returning as output.\n",
    "        return_inverse (bool): Whether to also return the indices for where\n",
    "            elements in the original input ended up in the returned unique list.\n",
    "        dim (int): the dimension to apply unique. If ``None``, the unique of the\n",
    "            flattened input is returned. default: ``None``\n",
    "\n",
    "    Returns:\n",
    "        (Tensor, Tensor (optional)): A tensor or a tuple of tensors containing\n",
    "\n",
    "            - **output** (*Tensor*): the output list of unique scalar elements.\n",
    "            - **inverse_indices** (*Tensor*): (optional) if\n",
    "              :attr:`return_inverse` is True, there will be a\n",
    "              2nd returned tensor (same shape as input) representing the indices\n",
    "              for where elements in the original input map to in the output;\n",
    "              otherwise, this function will only return a single tensor.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n",
    "        >>> output\n",
    "        tensor([ 2,  3,  1])\n",
    "\n",
    "        >>> output, inverse_indices = torch.unique(\n",
    "                torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n",
    "        >>> output\n",
    "        tensor([ 1,  2,  3])\n",
    "        >>> inverse_indices\n",
    "        tensor([ 0,  2,  1,  2])\n",
    "\n",
    "        >>> output, inverse_indices = torch.unique(\n",
    "                torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n",
    "        >>> output\n",
    "        tensor([ 1,  2,  3])\n",
    "        >>> inverse_indices\n",
    "        tensor([[ 0,  2],\n",
    "                [ 1,  2]])\n",
    "\n",
    "    \"\"\"\n",
    "    if dim is not None:\n",
    "        output, inverse_indices = torch._unique_dim(\n",
    "            input,\n",
    "            dim,\n",
    "            sorted=sorted,\n",
    "            return_inverse=return_inverse\n",
    "        )\n",
    "    else:\n",
    "        output, inverse_indices = torch._unique(\n",
    "            input,\n",
    "            sorted=sorted,\n",
    "            return_inverse=return_inverse,\n",
    "        )\n",
    "    if return_inverse:\n",
    "        return output, inverse_indices\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -n torch.unique\n",
    "def unique(input, sorted=False, return_inverse=False, dim=None):\n",
    "    r\"\"\"Returns the unique scalar elements of the input tensor as a 1-D tensor.\n",
    "\n",
    "    Arguments:\n",
    "        input (Tensor): the input tensor\n",
    "        sorted (bool): Whether to sort the unique elements in ascending order\n",
    "            before returning as output.\n",
    "        return_inverse (bool): Whether to also return the indices for where\n",
    "            elements in the original input ended up in the returned unique list.\n",
    "        dim (int): the dimension to apply unique. If ``None``, the unique of the\n",
    "            flattened input is returned. default: ``None``\n",
    "\n",
    "    Returns:\n",
    "        (Tensor, Tensor (optional)): A tensor or a tuple of tensors containing\n",
    "\n",
    "            - **output** (*Tensor*): the output list of unique scalar elements.\n",
    "            - **inverse_indices** (*Tensor*): (optional) if\n",
    "              :attr:`return_inverse` is True, there will be a\n",
    "              2nd returned tensor (same shape as input) representing the indices\n",
    "              for where elements in the original input map to in the output;\n",
    "              otherwise, this function will only return a single tensor.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n",
    "        >>> output\n",
    "        tensor([ 2,  3,  1])\n",
    "\n",
    "        >>> output, inverse_indices = torch.unique(\n",
    "                torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n",
    "        >>> output\n",
    "        tensor([ 1,  2,  3])\n",
    "        >>> inverse_indices\n",
    "        tensor([ 0,  2,  1,  2])\n",
    "\n",
    "        >>> output, inverse_indices = torch.unique(\n",
    "                torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n",
    "        >>> output\n",
    "        tensor([ 1,  2,  3])\n",
    "        >>> inverse_indices\n",
    "        tensor([[ 0,  2],\n",
    "                [ 1,  2]])\n",
    "\n",
    "    \"\"\"\n",
    "    if dim is not None:\n",
    "        output, inverse_indices = torch._unique_dim(\n",
    "            input,\n",
    "            dim,\n",
    "            sorted=sorted,\n",
    "            return_inverse=return_inverse\n",
    "        )\n",
    "    else:\n",
    "        output, inverse_indices = torch._unique(\n",
    "            input,\n",
    "            sorted=sorted,\n",
    "            return_inverse=return_inverse,\n",
    "        )\n",
    "    if return_inverse:\n",
    "        return output, inverse_indices\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
